---
title: "Logistic regression"
output:
  html_notebook:
    toc: true
    toc_float: true
---

## Objectives

Introduce basic functions to perform logistic regression using R.

## Basics of logistic regression and data

Logistisc regression is a kind og linear regression on binary data.

The interpretations of a linear regression is: For a given x, y increases by...

The effect of the explanatory variables is associtated by the other variables in the model, and the effect is linear.

However, when we are working with other variables, such as binary or factor variables.

First of all, lets just take a quick look at probabilities:

- p = the probability that somthing occurs.
- Odds = p / (1 − p)
  - p = 0.5 ⇒ Odds = 0.5/0.5 = 1

Another way to describe odds is by taking the logarithmic value of odds (logit)

- Logit = Ln(Odds) = Ln(p / (1 − p))
- Odds = e^logit
- p = e^logit / (1 - e^logit)

### Sample size
logit models require significant number of cases because they use maximum likelihood estimation techniques. It is also important to keep in mind that when the outcome is rare, even if the overall dataset is large, it can be difficult to estimate a logit model.

### Interpretation

Odds-ratio refers to the difference in odds for getting the response variable or not between different factors of a risk factor/explanatory variable.

Our H0 (null-hypothesis) states that there is no association between a risk factor and the outcome variable.


```{r message=FALSE}
library(readr)  # Reading data.
library(tibble)  # The "tibble" datastructure.
library(dplyr)  # Working with the tibble datastructure.
library(ggplot2)  # Plotting.
library(lubridate)  # Working with datetime objects.
library(magrittr)  # Need this for the %>% operator.
library(devtools)  # For printing the session info at the end of the notebook.
library(knitr)
library(aod)
```


We will go back to the data loaded previously ("weight_height_data.xlsx"). The data was loaded as a dataset called "data"" and manupilated previously. We do not have a binomial data in our data set, so this case just serves as an example. First of all, to create a binomial response variable we would like to create a variable that describes if Height_2 is equal to or higher than 138 cm (h2 < 138 cm = 0, h2 >= 138 cm = 1).

The outcome variable is now a continous variable, however our hypothesis is stated as if the variable was a binary variable. So we have to change it to a binary variable. The predictor variables are continous and dicotom variables (h1, w0, gender).

```{r}
data <- read_csv('weight_height_cleanup.csv')
head(data)
```

We will create the dicotome variable 'h2d' based on the height data from measurement 2 ('h2')
```{r}
data <- mutate(data, h2d = ifelse(h2 < 1.38, 0,
                    ifelse(h2 >= 1.38, 1, 2)))
```

```{r}
summary(data)
sapply(data,sd)
```

### two-way contigency table

- You should check for empty or small cells by doing a crosstab between categorical predictors and the outcome variable. If a cell has very few cases (a small cell), the model may become unstable or it might not run at all. Better to remove variables with small or no cases. Hence, we create a two-way contigency table:

```{r}
xtabs(~h2d + gender, data=data)
```

### logit model

First, lets change the diicotom variable to a factor variable:

```{r}
data$h2d <- factor(data$h2d)

```

Then, lets use the 'glm' function (generalized linear model).

First of all we specifiy the function. we want to see if height at birth can predict if a person gets higher than 1.38.

```{r}
logit <- glm(h2d ~ h0, data=data, family = "binomial")
summary(logit)
```

-First part of the output shows what was run.
-Next we see deviance residuals.
- Then we are presented with the actual model. The coefficients, their standard errors, the z-statistic (Wald z-statistic), and the associated p-values.
- Each one-unit change in age will increase the log odds of getting higher than 1.38 by 0.52908, however, the p-value indicates that it is insignificant.
- The difference between Null deviance and Residual deviance tells us that the model is a good fit. Greater the difference better the model.
- Null deviance is the value when you only have intercept in your equation with no variables and Residual deviance is the value when you are taking all the variables into account. It makes sense to consider the model good if that difference is big enough.


If we want to plot the data and the logistic regression model we can do this using ggplot:

```{r warning=FALSE}
ggplot(data, aes(x=h1, y=h2d)) + geom_point() + 
  stat_smooth(method="glm", method.args=list(family="binomial"), se=FALSE)
```

Now we will create a more complicated model with more predictors.

```{r}
logit <- glm(h2d ~ w0 + h0 + w1 + h1 + age2 + gender, data=data, family = "binomial")
summary(logit)
```

We can also add CIs using profiled log-likelihood or using standard errors

```{r}
confint(logit)
```


You can also exponentiate the coefficients and interpret them as odds-ratios. R will do this computation for you. To get the exponentiated coefficients, you tell R that you want to exponentiate (exp), and that the object you want to exponentiate is called coefficients and it is part of mylogit (coef(mylogit)). We can use the same logic to get odds ratios and their confidence intervals, by exponentiating the confidence intervals from before. To put it all in one table, we use cbind to bind the coefficients and confidence intervals column-wise.


```{r}
exp(cbind(OR = coef(logit), confint(logit)))
```


It is possible to test for interactions when there are multiple predictors. The interactions can be specified individually, as with a + b + c + a:b + b:c + a:b:c, or they can be expanded automatically, with a * b * c.

In this case we chose to perform a single interaction.

```{r}
logit_int <- glm(h2d ~ w0 + h0 + w1 + h1 + age2 + gender + h0:w0 + w1:h1 + h0:h1, data=data, family = "binomial")
summary(logit)
```

It was probably not a very good idea in this case since none of the interaction links are significant and they do not make much sense in this case. It was mostly to show it in the case.

We will go back to the previous model.

To return to the overall fit of the model we can test the difference between null deviance and residual deviance, the difference in degrees of freedoms and the p-value:

```{r}
with(logit, null.deviance - deviance)
with(logit, df.null - df.residual)
with(logit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

The chi-square of 153 with 7 degrees of freedom and an associated p-value of <0.001 tells us that our model as a whole fits significantly better than an empty model. This is sometimes called a likelihood ratio test (the deviance residual is -2*log likelihood). To see the model’s log likelihood, we type:

```{r}
logLik(logit)
```

We will now create a factor. None of the parameters in the dataset really works as a factor, so we will just create a random factor variable. Imagine that it is a variable of the childrens parents educational status.

```{r}
f <- floor(runif(444, min=1, max=5))
data = data %>% mutate(f=as.factor(f))
```

Furthermore, we will remove data that we are not using:

```{r}
data <- select(data, -bmi1)
data <- select(data, -age1)
head(data, n=5L)
```


```{r}
logit <- glm(h2d ~ w0 + h0 + w1 + h1 + age2 + gender + f, data=data, family = "binomial")
summary(logit)
confint(logit)
```

The indicator variables for rank have a slightly different interpretation. For example, having the kids parents a educational level of with rank of 2 versus a rank of 1, changes the log odds of the children being taller than 1.38 cm by -x.xxx (different every time we run the script as we have generated the rank randomly).


You can also exponentiate the coefficients again:

```{r}
exp(cbind(OR = coef(logit), confint(logit)))
```


```{r}
devtools::session_info()
```
